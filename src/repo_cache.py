"""
ì›ê²© ì €ì¥ì†Œ í´ë¡  ìºì‹œ ê´€ë¦¬ì
ì„¸ì…˜ ë‚´ì—ì„œ ë™ì¼ ì›ê²© ì €ì¥ì†Œì˜ ë¡œì»¬ í´ë¡ ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
ìºì‹œëŠ” JSON íŒŒì¼ë¡œ ì˜êµ¬ ì €ì¥ë˜ë©°, ë§Œë£Œ ì‹œê°„ì€ 1ì¼ì…ë‹ˆë‹¤.
"""

import os
import tempfile
import shutil
import hashlib
import logging
import json
import time
from typing import Optional, Dict
from pathlib import Path
from datetime import datetime, timedelta
import git

logger = logging.getLogger(__name__)


class RepoCloneCache:
    """ì›ê²© ì €ì¥ì†Œ í´ë¡  ìºì‹œ ì‹±ê¸€í†¤"""

    _instance = None
    _cache: Dict[str, Dict] = {}  # {cache_key: {url, path, created_at, last_accessed}}
    _cache_dir: Optional[str] = None
    _cache_file: Optional[str] = None
    _expire_days: int = 1  # ìºì‹œ ë§Œë£Œ ê¸°ê°„ (ì¼)

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
        return cls._instance

    @classmethod
    def reset_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°•ì œë¡œ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹… ìš©)"""
        cls._instance = None
        logger.info("Singleton instance reset")

    def _initialize(self):
        """ìºì‹œ ë””ë ‰í† ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”"""
        # Azure í™˜ê²½ ê°ì§€ ë° ì ì ˆí•œ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        cache_root = self._get_cache_root()
        self._cache_dir = str(cache_root / 'repos')
        self._cache_file = str(cache_root / 'cache_metadata.json')

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self._cache_dir, exist_ok=True)
        os.makedirs(os.path.dirname(self._cache_file), exist_ok=True)

        logger.info(f"Cache root: {cache_root}")
        logger.info(f"Initialized repository cache at: {self._cache_dir}")

        # Azure í™˜ê²½ì—ì„œ Git safe.directory ì„¤ì •
        self._configure_git_safe_directory()

        # ê¸°ì¡´ ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self._load_cache_metadata()

        # ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬ë§Œ ìˆ˜í–‰ (ë§Œë£Œ/ì†ìƒëœ ìºì‹œë§Œ ì œê±°)
        self._quick_validate_cache()

    def _get_cache_root(self) -> Path:
        """
        í™˜ê²½ì— ë§ëŠ” ìºì‹œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²°ì •

        Returns:
            Path: ìºì‹œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        """
        # í™˜ê²½ ë³€ìˆ˜ë¡œ ìºì‹œ ë””ë ‰í† ë¦¬ ëª…ì‹œì  ì§€ì • ê°€ëŠ¥
        if 'REPO_CACHE_DIR' in os.environ:
            cache_dir = Path(os.environ['REPO_CACHE_DIR'])
            logger.info(f"Using cache dir from REPO_CACHE_DIR: {cache_dir}")
            return cache_dir

        # Azure Web App í™˜ê²½ ê°ì§€
        if os.path.exists('/home/site/wwwroot'):
            # Azureì—ì„œëŠ” HOME í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© (ì¼ë°˜ì ìœ¼ë¡œ /home)
            home_dir = os.environ.get('HOME', '/home')
            cache_dir = Path(home_dir) / '.cache'
            logger.info(f"Azure environment detected, HOME={home_dir}, using: {cache_dir}")
            return cache_dir

        # Linux í™˜ê²½ì—ì„œëŠ” /tmp ì‚¬ìš© (ì„ íƒì ìœ¼ë¡œ ì˜êµ¬ ë””ë ‰í† ë¦¬)
        if os.name == 'posix':
            # ë¨¼ì € $HOME/.cache ì‹œë„ (ì˜êµ¬ì )
            if 'HOME' in os.environ:
                cache_dir = Path(os.environ['HOME']) / '.cache' / 'git_history_gen'
                logger.info(f"Using HOME cache dir: {cache_dir}")
                return cache_dir
            # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ /tmp (ì¬ì‹œì‘ ì‹œ ì‚­ì œë¨)
            cache_dir = Path(tempfile.gettempdir()) / 'git_history_gen_cache'
            logger.info(f"Using temp cache dir: {cache_dir}")
            return cache_dir

        # Windowsë‚˜ ê¸°íƒ€ í™˜ê²½ì—ì„œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì‚¬ìš©
        project_root = Path(__file__).parent.parent.resolve()
        cache_dir = project_root / '.cache'
        logger.info(f"Using project cache dir: {cache_dir}")
        return cache_dir

    def _configure_git_safe_directory(self):
        """
        Git safe.directory ì„¤ì • (Azure í™˜ê²½ ë“±ì—ì„œ ì†Œìœ ê¶Œ ë¬¸ì œ ë°©ì§€)
        - ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í•­ëª©ì€ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        - '*'ê°€ ì—†ìœ¼ë©´ í•œ ë²ˆë§Œ ì¶”ê°€
        - ê¸°ì¡´ ì¤‘ë³µ í•­ëª©ì´ ìˆë‹¤ë©´ ì •ë¦¬
        """
        try:
            import subprocess

            existing = self._get_safe_directories()
            if '*' not in existing:
                subprocess.run(
                    ['git', 'config', '--global', '--add', 'safe.directory', '*'],
                    capture_output=True,
                    timeout=5,
                    text=True,
                    check=False,
                )
                logger.info("âœ“ Configured Git safe.directory: *")
            else:
                logger.debug("safe.directory already contains '*'")

            # ì¤‘ë³µ ì •ë¦¬ (ëª¨ë“  í•­ëª© ëŒ€ìƒ, ìˆœì„œ ìœ ì§€)
            self._cleanup_safe_directory_duplicates()

        except FileNotFoundError:
            logger.warning("Git command not found, skipping safe.directory configuration")
        except Exception as e:
            logger.warning(f"Failed to configure safe.directory (non-critical): {e}")

    def _get_safe_directories(self) -> list[str]:
        """í˜„ì¬ ì„¤ì •ëœ safe.directory ëª©ë¡ì„ ë°˜í™˜"""
        try:
            import subprocess
            result = subprocess.run(
                ['git', 'config', '--global', '--get-all', 'safe.directory'],
                capture_output=True,
                timeout=5,
                text=True,
                check=False,
            )
            if result.returncode == 0 and result.stdout:
                entries = [line.strip() for line in result.stdout.splitlines() if line.strip()]
                return entries
        except Exception as e:
            logger.debug(f"Failed to read safe.directory: {e}")
        return []

    def _cleanup_safe_directory_duplicates(self):
        """safe.directory í•­ëª© ì¤‘ë³µì„ ì œê±°í•˜ê³  ìœ ì¼ í•­ëª©ë§Œ ë³´ì¡´"""
        try:
            import subprocess
            entries = self._get_safe_directories()
            if not entries:
                return

            unique = []
            seen = set()
            for e in entries:
                key = os.path.normcase(os.path.normpath(e)) if e != '*' else '*'
                if key not in seen:
                    seen.add(key)
                    unique.append(e)

            if unique == entries:
                return  # ì´ë¯¸ ì¤‘ë³µ ì—†ìŒ

            # ì „ì²´ í•­ëª© ì œê±° í›„ ìœ ë‹ˆí¬ í•­ëª©ë§Œ ì¬ë“±ë¡
            subprocess.run(
                ['git', 'config', '--global', '--unset-all', 'safe.directory'],
                capture_output=True,
                timeout=5,
                text=True,
                check=False,
            )

            for e in unique:
                subprocess.run(
                    ['git', 'config', '--global', '--add', 'safe.directory', e],
                    capture_output=True,
                    timeout=5,
                    text=True,
                    check=False,
                )
            logger.info(f"âœ“ Deduplicated safe.directory entries: {len(entries)} -> {len(unique)}")
        except FileNotFoundError:
            logger.debug("Git not found while deduplicating safe.directory")
        except Exception as e:
            logger.debug(f"Failed to deduplicate safe.directory: {e}")

    def _add_safe_directory(self, repo_path: str):
        """
        íŠ¹ì • ì €ì¥ì†Œ ê²½ë¡œë¥¼ Git safe.directoryì— ì¶”ê°€
        - '*'ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ê°œë³„ ê²½ë¡œ ì¶”ê°€ ë¶ˆí•„ìš”
        - ë™ì¼ ê²½ë¡œê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        Args:
            repo_path: ì €ì¥ì†Œ ê²½ë¡œ
        """
        try:
            import subprocess

            entries = self._get_safe_directories()
            # '*'ê°€ ìˆìœ¼ë©´ ëª¨ë“  ë””ë ‰í† ë¦¬ í—ˆìš©ë¨ â†’ ìŠ¤í‚µ
            if '*' in entries:
                logger.debug("'*' present in safe.directory; skipping per-repo add")
                return

            # ê²½ë¡œ ì •ê·œí™”ë¡œ ì¤‘ë³µ ë°©ì§€ (Windows ëŒ€ì†Œë¬¸ì/ìŠ¬ë˜ì‹œ ì°¨ì´ ë³´ì •)
            target = os.path.normcase(os.path.normpath(repo_path))
            normalized_set = {os.path.normcase(os.path.normpath(e)) for e in entries}

            if target in normalized_set:
                logger.debug(f"safe.directory already contains: {repo_path}")
                return

            result = subprocess.run(
                ['git', 'config', '--global', '--add', 'safe.directory', repo_path],
                capture_output=True,
                timeout=5,
                text=True,
                check=False,
            )

            if result.returncode == 0:
                logger.info(f"âœ“ Added safe.directory: {repo_path}")
            else:
                logger.debug(f"Failed to add safe.directory (non-critical): {result.stderr}")
        except FileNotFoundError:
            logger.debug("Git command not found")
        except Exception as e:
            logger.debug(f"Failed to add safe.directory (non-critical): {e}")

    def _ensure_commit_exists(self, repo_path: str, repo_url: str, commit_sha: str) -> bool:
        """
        íŠ¹ì • ì»¤ë°‹ì´ ë¡œì»¬ ì €ì¥ì†Œì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ fetch

        Args:
            repo_path: ë¡œì»¬ ì €ì¥ì†Œ ê²½ë¡œ
            repo_url: ì›ê²© ì €ì¥ì†Œ URL
            commit_sha: í™•ì¸í•  ì»¤ë°‹ SHA

        Returns:
            bool: ì»¤ë°‹ì´ ì¡´ì¬í•˜ë©´ True
        """
        try:
            repo = git.Repo(repo_path)

            # ì»¤ë°‹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            try:
                repo.commit(commit_sha)
                logger.debug(f"Commit {commit_sha[:8]} exists locally")
                return True
            except (git.exc.BadName, ValueError):
                # ì»¤ë°‹ì´ ì—†ìœ¼ë©´ fetch ì‹œë„
                logger.info(f"Commit {commit_sha[:8]} not found, fetching...")

                origin = repo.remotes.origin

                # íŠ¹ì • ì»¤ë°‹ì„ í¬í•¨í•˜ë„ë¡ ë” ê¹Šê²Œ fetch
                try:
                    # ë¨¼ì € depthë¥¼ ëŠ˜ë ¤ì„œ fetch
                    origin.fetch(depth=1000)

                    # ë‹¤ì‹œ í™•ì¸
                    try:
                        repo.commit(commit_sha)
                        logger.info(f"âœ“ Fetched commit {commit_sha[:8]}")
                        return True
                    except:
                        # ì—¬ì „íˆ ì—†ìœ¼ë©´ ì „ì²´ íˆìŠ¤í† ë¦¬ fetch
                        logger.info(f"Fetching full history for commit {commit_sha[:8]}...")
                        origin.fetch(unshallow=True)

                        repo.commit(commit_sha)
                        logger.info(f"âœ“ Fetched commit {commit_sha[:8]} (full history)")
                        return True

                except Exception as e:
                    logger.warning(f"Failed to fetch commit {commit_sha[:8]}: {e}")
                    return False

        except Exception as e:
            logger.error(f"Error checking commit existence: {e}")
            return False

    def _load_cache_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ì—ì„œ ë¡œë“œ"""
        if os.path.exists(self._cache_file):
            try:
                with open(self._cache_file, 'r', encoding='utf-8') as f:
                    self._cache = json.load(f)
                logger.info(f"Loaded cache metadata: {len(self._cache)} entries")
            except Exception as e:
                logger.warning(f"Failed to load cache metadata: {e}")
                self._cache = {}
        else:
            logger.info("No existing cache metadata found")
            self._cache = {}

    def _save_cache_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self._cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, indent=2, ensure_ascii=False)
            logger.debug("Saved cache metadata")
        except Exception as e:
            logger.error(f"Failed to save cache metadata: {e}")

    def _quick_validate_cache(self):
        """ë¹ ë¥¸ ìºì‹œ ê²€ì¦ (ë§Œë£Œ ë° ê²½ë¡œ ì¡´ì¬ë§Œ ì²´í¬, fetchëŠ” ì•ˆí•¨)"""
        logger.info("Quick validating cache entries...")

        invalid_keys = []
        expired_keys = []
        now = datetime.now()

        for cache_key, entry in self._cache.items():
            try:
                cache_path = entry['path']
                cache_path = self._normalize_cache_path(cache_path, cache_key)
                entry['path'] = cache_path

                created_at = datetime.fromisoformat(entry['created_at'])
                repo_url = entry['url']

                # ë§Œë£Œ í™•ì¸
                if now - created_at > timedelta(days=self._expire_days):
                    logger.info(f"Cache expired: {repo_url}")
                    expired_keys.append(cache_key)
                    continue

                # ê²½ë¡œ ì¡´ì¬ í™•ì¸ë§Œ
                if not os.path.exists(cache_path):
                    logger.warning(f"Cache path not found: {cache_path}")
                    invalid_keys.append(cache_key)
                    continue

                # Git ì €ì¥ì†Œ ìœ íš¨ì„±ë§Œ í™•ì¸ (fetchëŠ” ì•ˆí•¨)
                try:
                    repo = git.Repo(cache_path)
                    repo.close()
                    logger.debug(f"âœ“ Valid cache entry: {repo_url}")
                except Exception as e:
                    logger.warning(f"Invalid git repository: {cache_path} - {e}")
                    invalid_keys.append(cache_key)

            except Exception as e:
                logger.warning(f"Failed to validate cache entry {cache_key}: {e}")
                invalid_keys.append(cache_key)

        # ë§Œë£Œ/ì†ìƒëœ ìºì‹œë§Œ ì •ë¦¬
        removed_count = 0
        for cache_key in expired_keys + invalid_keys:
            self._invalidate_cache(cache_key)
            removed_count += 1

        if removed_count > 0:
            logger.info(f"Cleaned up {removed_count} invalid/expired cache entries")
            self._save_cache_metadata()
        else:
            logger.info("All cache entries are valid")

    def _validate_single_repo(self, cache_key: str) -> bool:
        """
        íŠ¹ì • ì €ì¥ì†Œì˜ ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì—…ë°ì´íŠ¸

        Args:
            cache_key: ìºì‹œ í‚¤

        Returns:
            bool: ìœ íš¨í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        if cache_key not in self._cache:
            return False

        try:
            entry = self._cache[cache_key]
            cache_path = entry['path']
            repo_url = entry['url']

            # safe.directory ì„¤ì •
            self._add_safe_directory(cache_path)

            # Git ì €ì¥ì†Œ ìœ íš¨ì„± í™•ì¸ ë° ì—…ë°ì´íŠ¸
            repo = git.Repo(cache_path)
            logger.info(f"Fetching latest changes for: {repo_url}")
            origin = repo.remotes.origin
            origin.fetch()
            repo.git.reset('--hard', 'origin/HEAD')

            # ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸
            entry['last_accessed'] = datetime.now().isoformat()
            self._save_cache_metadata()

            logger.info(f"âœ“ Updated cache entry: {repo_url}")
            return True

        except Exception as e:
            logger.warning(f"Failed to validate cache entry {cache_key}: {e}")
            return False

    def _validate_and_cleanup_cache(self):
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ë° ë§Œë£Œ/ì†ìƒëœ ìºì‹œ ì •ë¦¬ (ì „ì²´ ê²€ì¦ - ì‚¬ìš© ì•ˆí•¨)"""
        logger.info("Validating cache entries...")

        invalid_keys = []
        expired_keys = []
        now = datetime.now()

        for cache_key, entry in self._cache.items():
            try:
                cache_path = entry['path']
                # ê²½ë¡œ ì •ê·œí™”
                cache_path = self._normalize_cache_path(cache_path, cache_key)
                entry['path'] = cache_path  # ì •ê·œí™”ëœ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸

                created_at = datetime.fromisoformat(entry['created_at'])
                repo_url = entry['url']

                # ë§Œë£Œ í™•ì¸
                if now - created_at > timedelta(days=self._expire_days):
                    logger.info(f"Cache expired: {repo_url} (age: {(now - created_at).days} days)")
                    expired_keys.append(cache_key)
                    continue

                # ê²½ë¡œ ì¡´ì¬ í™•ì¸
                if not os.path.exists(cache_path):
                    logger.warning(f"Cache path not found: {cache_path}")
                    invalid_keys.append(cache_key)
                    continue

                # Git ì €ì¥ì†Œ ìœ íš¨ì„± í™•ì¸ ë° ì—…ë°ì´íŠ¸
                try:
                    repo = git.Repo(cache_path)

                    # ìœ íš¨í•œ ìºì‹œëŠ” ìµœì‹ í™” ì‹œë„
                    logger.info(f"Updating cached repository: {repo_url}")
                    origin = repo.remotes.origin
                    origin.fetch()
                    repo.git.reset('--hard', 'origin/HEAD')

                    # ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸
                    entry['last_accessed'] = now.isoformat()

                    logger.info(f"âœ“ Valid cache entry: {repo_url}")

                except git.exc.GitCommandError as e:
                    logger.warning(f"Git operation failed for {cache_path}: {e}")
                    invalid_keys.append(cache_key)
                except Exception as e:
                    logger.warning(f"Invalid git repository: {cache_path} - {e}")
                    invalid_keys.append(cache_key)

            except Exception as e:
                logger.warning(f"Failed to validate cache entry {cache_key}: {e}")
                invalid_keys.append(cache_key)

        # ë§Œë£Œ/ì†ìƒëœ ìºì‹œ ì •ë¦¬
        removed_count = 0
        for cache_key in expired_keys + invalid_keys:
            self._invalidate_cache(cache_key)
            removed_count += 1

        if removed_count > 0:
            logger.info(f"Cleaned up {removed_count} invalid/expired cache entries")
            self._save_cache_metadata()
        else:
            logger.info("All cache entries are valid")

    def _get_cache_key(self, repo_url: str) -> str:
        """ì €ì¥ì†Œ URLì—ì„œ ìºì‹œ í‚¤ ìƒì„±"""
        url_hash = hashlib.md5(repo_url.encode()).hexdigest()[:12]
        return url_hash

    def _normalize_cache_path(self, path: str, cache_key: str) -> str:
        """
        ìºì‹œ ê²½ë¡œë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤. ì˜ëª»ëœ ê²½ë¡œ íŒ¨í„´ì„ ê°ì§€í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.

        Args:
            path: ì›ë³¸ ê²½ë¡œ
            cache_key: ìºì‹œ í‚¤

        Returns:
            ì •ê·œí™”ëœ ê²½ë¡œ
        """
        # ì˜ëª»ëœ íŒ¨í„´ ê°ì§€: git_history_gen.cache (ë°±ìŠ¬ë˜ì‹œ ì—†ì´ ì ìœ¼ë¡œ ë¶™ì–´ìˆìŒ)
        if 'git_history_gen.cache' in path:
            logger.warning(f"Detected malformed path pattern: {path}")
            # ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •
            correct_path = os.path.join(self._cache_dir, cache_key)
            logger.info(f"Corrected path: {correct_path}")
            return correct_path

        # ê²½ë¡œê°€ _cache_dirë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¬êµ¬ì„±
        if not path.startswith(self._cache_dir):
            logger.warning(f"Path does not start with cache_dir: {path}")
            correct_path = os.path.join(self._cache_dir, cache_key)
            logger.info(f"Reconstructed path: {correct_path}")
            return correct_path

        return path

    def _force_remove_directory(self, path: str, max_retries: int = 3):
        """
        ë””ë ‰í† ë¦¬ë¥¼ ê°•ì œë¡œ ì‚­ì œí•©ë‹ˆë‹¤ (ë‹¤ì¤‘ ì‹œë„).

        Args:
            path: ì‚­ì œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        import stat
        import subprocess

        for attempt in range(max_retries):
            try:
                if not os.path.exists(path):
                    return  # ì´ë¯¸ ì‚­ì œë¨

                logger.info(f"Attempting to remove directory (attempt {attempt + 1}/{max_retries}): {path}")

                # ì‹œë„ 1: ì¼ë°˜ ì‚­ì œ
                try:
                    shutil.rmtree(path)
                    time.sleep(0.3)  # Windows íŒŒì¼ì‹œìŠ¤í…œ ë™ê¸°í™” ëŒ€ê¸°
                    if not os.path.exists(path):
                        logger.info(f"âœ“ Successfully removed: {path}")
                        return
                    logger.debug(f"Directory still exists after normal removal")
                except Exception as e:
                    logger.debug(f"Normal removal failed: {e}")

                # ì‹œë„ 2: ì½ê¸° ì „ìš© ì†ì„± ì œê±° í›„ ì‚­ì œ
                try:
                    def remove_readonly(func, path, _):
                        try:
                            os.chmod(path, stat.S_IWRITE)
                            func(path)
                        except:
                            pass

                    time.sleep(0.5)  # íŒŒì¼ í•¸ë“¤ í•´ì œ ëŒ€ê¸°
                    shutil.rmtree(path, onerror=remove_readonly)
                    time.sleep(0.5)  # ì‚­ì œ ì™„ë£Œ ëŒ€ê¸°

                    if not os.path.exists(path):
                        logger.info(f"âœ“ Force-removed with readonly fix: {path}")
                        return
                    logger.debug(f"Directory still exists after readonly removal")
                except Exception as e:
                    logger.debug(f"Readonly removal failed: {e}")

                # ì‹œë„ 3: ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½ í›„ ì‚­ì œ (Windowsì—ì„œ íš¨ê³¼ì )
                try:
                    import uuid
                    temp_name = os.path.join(os.path.dirname(path), f"_temp_delete_{uuid.uuid4().hex[:8]}")

                    # ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½ (íŒŒì¼ì´ ì ê²¨ìˆì–´ë„ ê°€ëŠ¥)
                    os.rename(path, temp_name)
                    logger.info(f"Renamed directory: {path} -> {temp_name}")

                    time.sleep(0.5)

                    # ì´ë¦„ ë°”ê¾¼ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹œë„
                    try:
                        shutil.rmtree(temp_name, ignore_errors=True)
                        time.sleep(0.5)  # ì‚­ì œ ì™„ë£Œ ëŒ€ê¸°
                    except:
                        pass

                    if not os.path.exists(path):
                        logger.info(f"âœ“ Removed using rename strategy: {path}")
                        # ì´ë¦„ ë°”ê¾¼ ë””ë ‰í† ë¦¬ê°€ ë‚¨ì•„ìˆë‹¤ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹œë„
                        if os.path.exists(temp_name):
                            logger.warning(f"Renamed directory still exists, will retry: {temp_name}")
                            # ë¹„ë™ê¸° ì‚­ì œ ì¬ì‹œë„ (ë¬´ì‹œ)
                            try:
                                shutil.rmtree(temp_name, ignore_errors=True)
                            except:
                                pass
                        return
                    logger.debug(f"Original path still exists after rename strategy")
                except Exception as e:
                    logger.debug(f"Rename removal failed: {e}")

                # ì‹œë„ 4: Windows robocopyë¡œ ë¹ˆ ë””ë ‰í† ë¦¬ ë®ì–´ì“°ê¸° í›„ ì‚­ì œ
                try:
                    temp_empty = tempfile.mkdtemp()
                    try:
                        # robocopy: ë¹ˆ ë””ë ‰í† ë¦¬ë¡œ ë®ì–´ì“°ê¸° (íŒŒì¼ ì‚­ì œ íš¨ê³¼)
                        result = subprocess.run(
                            ['robocopy', temp_empty, path, '/mir', '/w:1', '/r:1'],
                            capture_output=True,
                            timeout=30
                        )
                        # robocopy exit code 0-7ì€ ì„±ê³µ
                        if result.returncode <= 7:
                            time.sleep(0.5)
                            shutil.rmtree(path, ignore_errors=True)
                            shutil.rmtree(temp_empty, ignore_errors=True)
                            logger.info(f"âœ“ Removed using robocopy: {path}")
                            return
                    finally:
                        try:
                            shutil.rmtree(temp_empty, ignore_errors=True)
                        except:
                            pass
                except Exception as e:
                    logger.debug(f"Robocopy removal failed: {e}")

                # ì‹œë„ 5: rmdir ì‹œìŠ¤í…œ ëª…ë ¹
                try:
                    result = subprocess.run(
                        ['cmd', '/c', 'rmdir', '/s', '/q', path],
                        capture_output=True,
                        timeout=30
                    )
                    time.sleep(0.5)
                    if not os.path.exists(path):
                        logger.info(f"âœ“ Removed using rmdir: {path}")
                        return
                except Exception as e:
                    logger.debug(f"Rmdir removal failed: {e}")

                # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                if attempt < max_retries - 1:
                    time.sleep(1)

            except Exception as e:
                logger.error(f"Attempt {attempt + 1} failed: {e}")

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.error(f"âš ï¸ Failed to remove directory after {max_retries} attempts: {path}")
        logger.error(f"âš ï¸ Manual cleanup required!")
        raise Exception(f"Cannot remove directory: {path}")

    def get_or_clone(self, repo_url: str, depth: Optional[int] = None, ensure_commit: Optional[str] = None) -> str:
        """
        ìºì‹œëœ í´ë¡ ì„ ë°˜í™˜í•˜ê±°ë‚˜ ìƒˆë¡œ í´ë¡ í•©ë‹ˆë‹¤.

        Args:
            repo_url: ì›ê²© ì €ì¥ì†Œ URL
            depth: clone depth (None=shallow clone with depth=50, 0=full history)
            ensure_commit: íŠ¹ì • ì»¤ë°‹ì´ í•„ìš”í•œ ê²½ìš° (ì—†ìœ¼ë©´ fetch)

        Returns:
            str: ë¡œì»¬ ì €ì¥ì†Œ ê²½ë¡œ
        """
        cache_key = self._get_cache_key(repo_url)
        now = datetime.now()

        # ì´ë¯¸ ìºì‹œëœ ê²½ìš°
        if cache_key in self._cache:
            entry = self._cache[cache_key]
            cached_path = entry['path']
            created_at = datetime.fromisoformat(entry['created_at'])

            # ë§Œë£Œ í™•ì¸
            if now - created_at > timedelta(days=self._expire_days):
                logger.info(f"Cache expired for {repo_url}, re-cloning...")
                self._invalidate_cache(cache_key)
            else:
                # ìºì‹œëœ ê²½ë¡œê°€ ìœ íš¨í•œì§€ í™•ì¸
                if os.path.exists(cached_path):
                    try:
                        # depth ìš”ì²­ì´ ìˆê³  ê¸°ì¡´ë³´ë‹¤ ê¹Šê²Œ í•„ìš”í•œ ê²½ìš°
                        if depth and depth > 50:  # ê¸°ë³¸ shallow depthë³´ë‹¤ í¬ë©´
                            logger.info(f"Fetching more commits (depth={depth})...")
                            self._add_safe_directory(cached_path)
                            repo = git.Repo(cached_path)
                            origin = repo.remotes.origin
                            # deepen fetch
                            origin.fetch(depth=depth)
                            repo.close()
                            logger.info(f"âœ“ Fetched more commits: {cached_path}")

                        # íŠ¹ì • ì»¤ë°‹ì´ í•„ìš”í•œ ê²½ìš° í™•ì¸
                        if ensure_commit:
                            if self._ensure_commit_exists(cached_path, repo_url, ensure_commit):
                                logger.info(f"âœ“ Cache hit with commit {ensure_commit[:8]}: {cached_path}")
                                return cached_path
                            else:
                                # ì»¤ë°‹ fetch ì‹œë„
                                logger.warning(f"Commit {ensure_commit[:8]} not found, will fetch")

                        # ì´ ì €ì¥ì†Œë§Œ ê²€ì¦ ë° ì—…ë°ì´íŠ¸
                        if self._validate_single_repo(cache_key):
                            logger.info(f"âœ“ Cache hit and updated: {cached_path}")
                            return cached_path
                        else:
                            # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬í´ë¡ 
                            logger.warning(f"Cache validation failed, will re-clone")
                            self._invalidate_cache(cache_key)

                    except Exception as e:
                        logger.warning(f"Cached repo invalid, will re-clone: {e}")
                        # ìºì‹œ ë¬´íš¨í™”
                        self._invalidate_cache(cache_key)

        # ìƒˆë¡œ í´ë¡ 
        logger.info(f"Cache miss, cloning: {repo_url}")
        local_path = os.path.join(self._cache_dir, cache_key)

        try:
            # ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ git pull ì‹œë„
            if os.path.exists(local_path):
                logger.info(f"Directory already exists: {local_path}")

                try:
                    # Git ì €ì¥ì†Œì¸ì§€ í™•ì¸í•˜ê³  pull ì‹œë„
                    logger.info(f"Attempting to use existing repo and pull...")

                    # safe.directory ì„¤ì • (Azure í™˜ê²½ì—ì„œ í•„ìš”)
                    self._add_safe_directory(local_path)

                    existing_repo = git.Repo(local_path)

                    # remote origin í™•ì¸
                    if 'origin' in [remote.name for remote in existing_repo.remotes]:
                        origin = existing_repo.remotes.origin

                        # fetch ë° reset (ì „ì²´ íˆìŠ¤í† ë¦¬)
                        logger.info(f"Fetching latest changes (full history)...")
                        origin.fetch()
                        existing_repo.git.reset('--hard', 'origin/HEAD')

                        logger.info(f"âœ“ Successfully updated existing repository")

                        # ìºì‹œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                        now = datetime.now()
                        self._cache[cache_key] = {
                            'url': repo_url,
                            'path': local_path,
                            'created_at': now.isoformat(),
                            'last_accessed': now.isoformat(),
                            'clone_depth': depth
                        }
                        self._save_cache_metadata()

                        return local_path
                    else:
                        logger.warning(f"No 'origin' remote found, will re-clone")
                        raise git.exc.GitCommandError('fetch', 'No origin remote')

                except (git.exc.InvalidGitRepositoryError, git.exc.GitCommandError) as e:
                    logger.warning(f"Cannot use existing directory ({type(e).__name__}), removing and re-cloning...")

                    # Git ì €ì¥ì†Œ ë¨¼ì € ë‹«ê¸°
                    try:
                        existing_repo.close()
                        del existing_repo
                    except Exception as close_err:
                        logger.debug(f"Could not close repo: {close_err}")

                    # ê°•ë ¥í•œ ì‚­ì œ ì‹œë„
                    self._force_remove_directory(local_path)

                    # ì‚­ì œ í™•ì¸ (ì—¬ëŸ¬ ë²ˆ ì¬ì‹œë„)
                    max_check_attempts = 5
                    for check_attempt in range(max_check_attempts):
                        if not os.path.exists(local_path):
                            logger.info(f"âœ“ Directory removal confirmed after {check_attempt + 1} checks")
                            break

                        if check_attempt < max_check_attempts - 1:
                            logger.debug(f"Directory still exists, waiting... (attempt {check_attempt + 1}/{max_check_attempts})")
                            time.sleep(0.5)
                    else:
                        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
                        raise Exception(f"Failed to remove directory completely: {local_path}")

            # ìƒˆë¡œ í´ë¡ 
            logger.info(f"Cloning fresh repository (full history)...")
            logger.warning(f"âš ï¸ Large repository detected. This may take several minutes...")

            # Windowsì—ì„œ ê¸´ ê²½ë¡œ ì§€ì› ì„¤ì •
            try:
                import subprocess
                # Git ê¸€ë¡œë²Œ ì„¤ì •ì—ì„œ longpaths í™œì„±í™”
                subprocess.run(['git', 'config', '--global', 'core.longpaths', 'true'],
                             capture_output=True, check=False)
                logger.debug("Enabled Git longpaths support")
            except Exception as e:
                logger.debug(f"Could not set git longpaths: {e}")

            # ì§„í–‰ ìƒí™© ì½œë°±
            class CloneProgress(git.RemoteProgress):
                def __init__(self):
                    super().__init__()
                    self.last_log_time = time.time()
                    self.message_obj = None
                    self.current_stage = None
                    self.stage_start_time = time.time()

                    # Chainlit ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
                    try:
                        import chainlit as cl
                        self.cl = cl
                        # í˜„ì¬ Chainlit ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if hasattr(cl.context, 'session') and cl.context.session:
                            self.use_chainlit = True
                        else:
                            self.use_chainlit = False
                    except:
                        self.cl = None
                        self.use_chainlit = False

                def _get_stage_name(self, op_code):
                    """ì‘ì—… ì½”ë“œì—ì„œ ë‹¨ê³„ ì´ë¦„ ì¶”ì¶œ"""
                    if op_code & self.COUNTING:
                        return "Counting objects"
                    elif op_code & self.COMPRESSING:
                        return "Compressing objects"
                    elif op_code & self.RECEIVING:
                        return "Receiving objects"
                    elif op_code & self.RESOLVING:
                        return "Resolving deltas"
                    elif op_code & self.FINDING_SOURCES:
                        return "Finding sources"
                    elif op_code & self.CHECKING_OUT:
                        return "Checking out files"
                    else:
                        return "Processing"

                def update(self, op_code, cur_count, max_count=None, message=''):
                    # í˜„ì¬ ë‹¨ê³„ í™•ì¸
                    stage = self._get_stage_name(op_code)

                    # ë‹¨ê³„ê°€ ë³€ê²½ë˜ë©´ ë¡œê·¸
                    if stage != self.current_stage:
                        if self.current_stage:
                            elapsed = time.time() - self.stage_start_time
                            logger.info(f"âœ“ {self.current_stage} completed in {elapsed:.1f}s")
                        self.current_stage = stage
                        self.stage_start_time = time.time()
                        logger.info(f"â–¶ {stage}...")

                    # 5ì´ˆë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê·¸
                    now = time.time()
                    if now - self.last_log_time >= 5:
                        if max_count and max_count > 0:
                            percentage = (cur_count / max_count * 100)
                            msg = f"  ğŸ”„ {stage}: {percentage:.1f}% ({cur_count:,}/{max_count:,})"
                        else:
                            msg = f"  ğŸ”„ {stage}: {cur_count:,} items"

                        logger.info(msg)

                        # Chainlit UIì—ë„ í‘œì‹œ
                        if self.use_chainlit and self.cl:
                            try:
                                import asyncio

                                async def send_or_update():
                                    if self.message_obj is None:
                                        # ì²« ë©”ì‹œì§€ ìƒì„±
                                        self.message_obj = await self.cl.Message(
                                            content=msg,
                                            author="System"
                                        ).send()
                                    else:
                                        # ê¸°ì¡´ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
                                        self.message_obj.content = msg
                                        await self.message_obj.update()

                                # ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
                                try:
                                    loop = asyncio.get_event_loop()
                                    if loop.is_running():
                                        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ì—ì„œëŠ” task ìƒì„±
                                        asyncio.create_task(send_or_update())
                                    else:
                                        # ìƒˆ ë£¨í”„ ì‹¤í–‰
                                        asyncio.run(send_or_update())
                                except:
                                    # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹œë„
                                    pass
                            except Exception as e:
                                logger.debug(f"Failed to send Chainlit message: {e}")

                        self.last_log_time = now

            # depth ì„¤ì • (ê¸°ë³¸ê°’: shallow clone)
            if depth is None:
                # ê¸°ë³¸: shallow clone (depth=50)
                clone_depth = 50
                logger.info(f"Using shallow clone (depth={clone_depth})")
            elif depth == 0:
                # ì „ì²´ íˆìŠ¤í† ë¦¬
                clone_depth = None
                logger.info("Using full history clone")
            else:
                clone_depth = depth
                logger.info(f"Using custom depth: {clone_depth}")

            clone_kwargs = {
                'single_branch': True,
                'progress': CloneProgress()  # ì§„í–‰ ìƒí™© ì¶”ê°€
            }

            if clone_depth:
                clone_kwargs['depth'] = clone_depth

            logger.info(f"Starting clone of {repo_url}...")

            # Chainlit UIì— ì‹œì‘ ë©”ì‹œì§€ ì „ì†¡
            try:
                import chainlit as cl
                if hasattr(cl.context, 'session') and cl.context.session:
                    import asyncio
                    async def send_start_msg():
                        await cl.Message(
                            content=f"ğŸ”„ ì €ì¥ì†Œ í´ë¡  ì‹œì‘: {repo_url}\nâš ï¸ í° ì €ì¥ì†ŒëŠ” ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                            author="System"
                        ).send()

                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            asyncio.create_task(send_start_msg())
                        else:
                            asyncio.run(send_start_msg())
                    except:
                        pass
            except:
                pass

            start_time = time.time()

            repo = git.Repo.clone_from(
                repo_url,
                local_path,
                **clone_kwargs
            )

            elapsed = time.time() - start_time
            logger.info(f"âœ“ Clone completed in {elapsed:.1f} seconds")

            # Chainlit UIì— ì™„ë£Œ ë©”ì‹œì§€ ì „ì†¡
            try:
                import chainlit as cl
                if hasattr(cl.context, 'session') and cl.context.session:
                    import asyncio
                    async def send_complete_msg():
                        await cl.Message(
                            content=f"âœ… ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ! ({elapsed:.1f}ì´ˆ)",
                            author="System"
                        ).send()

                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            asyncio.create_task(send_complete_msg())
                        else:
                            asyncio.run(send_complete_msg())
                    except:
                        pass
            except:
                pass

            # Azure í™˜ê²½ì—ì„œ safe.directory ì„¤ì • (í´ë¡  ì§í›„)
            self._add_safe_directory(local_path)

            # ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥
            self._cache[cache_key] = {
                'url': repo_url,
                'path': local_path,
                'created_at': now.isoformat(),
                'last_accessed': now.isoformat()
            }
            self._save_cache_metadata()

            logger.info(f"âœ“ Cloned and cached: {local_path}")

            return local_path

        except Exception as e:
            logger.error(f"Failed to clone repository: {e}")

            # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸´ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            error_msg = str(e)
            if 'Filename too long' in error_msg or 'unable to create file' in error_msg:
                logger.error("=" * 60)
                logger.error("Git Clone ì‹¤íŒ¨: íŒŒì¼ ê²½ë¡œê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤")
                logger.error("=" * 60)
                logger.error(f"ì €ì¥ì†Œ: {repo_url}")
                logger.error("\ní•´ê²° ë°©ë²•:")
                logger.error("1. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ PowerShell ì‹¤í–‰")
                logger.error("2. ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰: git config --system core.longpaths true")
                logger.error("3. ë˜ëŠ” Windows ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ê¸´ ê²½ë¡œ í™œì„±í™”:")
                logger.error("   HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem")
                logger.error("   LongPathsEnabled = 1 (DWORD)")
                logger.error("4. í”„ë¡œê·¸ë¨ ì¬ì‹œì‘")
                logger.error("=" * 60)

                # ì‹¤íŒ¨í•œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                if os.path.exists(local_path):
                    try:
                        import stat
                        def remove_readonly(func, path, _):
                            os.chmod(path, stat.S_IWRITE)
                            func(path)
                        shutil.rmtree(local_path, onerror=remove_readonly)
                    except:
                        pass

                raise Exception(
                    f"íŒŒì¼ ê²½ë¡œê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. Windowsì—ì„œ ê¸´ ê²½ë¡œ ì§€ì›ì„ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤. "
                    f"ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ 'git config --system core.longpaths true' ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”."
                ) from e

            # ì‹¤íŒ¨ ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if os.path.exists(local_path):
                shutil.rmtree(local_path, ignore_errors=True)
            raise

    def _invalidate_cache(self, cache_key: str):
        """ìºì‹œ ë¬´íš¨í™”"""
        if cache_key in self._cache:
            entry = self._cache[cache_key]
            cached_path = entry['path']

            # ê²½ë¡œ ì •ê·œí™” ë° ê²€ì¦
            cached_path = self._normalize_cache_path(cached_path, cache_key)

            if os.path.exists(cached_path):
                # 1ë‹¨ê³„: Git ì €ì¥ì†Œ ë‹«ê¸°
                try:
                    repo = git.Repo(cached_path)
                    repo.close()
                    del repo
                    logger.debug(f"Closed Git repo: {cached_path}")
                except Exception as e:
                    logger.debug(f"Could not close repo (may not be valid): {e}")

                # 2ë‹¨ê³„: ê°•ë ¥í•œ ì‚­ì œ
                try:
                    self._force_remove_directory(cached_path)
                except Exception as e:
                    logger.error(f"Failed to remove cached repo: {e}")

            del self._cache[cache_key]
            self._save_cache_metadata()

    def clear_all(self):
        """ëª¨ë“  ìºì‹œ ì •ë¦¬"""
        logger.info("Clearing all cached repositories...")

        for cache_key in list(self._cache.keys()):
            self._invalidate_cache(cache_key)

        if self._cache_dir and os.path.exists(self._cache_dir):
            try:
                shutil.rmtree(self._cache_dir, ignore_errors=True)
                logger.info(f"âœ“ Cleared cache directory: {self._cache_dir}")
            except Exception as e:
                logger.warning(f"Failed to clear cache directory: {e}")

        self._cache.clear()
        self._save_cache_metadata()

    def get_cache_info(self) -> Dict:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        info = {
            "cache_dir": self._cache_dir or "",
            "cache_file": self._cache_file or "",
            "cached_repos": len(self._cache),
            "expire_days": self._expire_days,
            "repos": []
        }

        now = datetime.now()
        for cache_key, entry in self._cache.items():
            created_at = datetime.fromisoformat(entry['created_at'])
            age_days = (now - created_at).days

            info["repos"].append({
                "url": entry['url'],
                "cache_key": cache_key,
                "age_days": age_days,
                "is_expired": age_days > self._expire_days
            })

        return info

    def __del__(self):
        """ì†Œë©¸ì - ë©”íƒ€ë°ì´í„° ì €ì¥ (ìºì‹œ ë””ë ‰í† ë¦¬ëŠ” ìœ ì§€)"""
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥, ìºì‹œ ë””ë ‰í† ë¦¬ëŠ” ìœ ì§€
        try:
            # jsonê³¼ openì´ ì•„ì§ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if hasattr(self, '_cache_file') and self._cache_file:
                import builtins
                if hasattr(builtins, 'open'):
                    self._save_cache_metadata()
        except Exception:
            # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì—ëŸ¬ëŠ” ë¬´ì‹œ
            pass

