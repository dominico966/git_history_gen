"""
Azure AI Search ì¸ë±ì‹± ëª¨ë“ˆ
ì»¤ë°‹ ë°ì´í„°ë¥¼ Azure AI Searchì— ì¸ë±ì‹±í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
"""

import logging
import hashlib
from typing import Optional, List, Set
from urllib.parse import urlparse
from pathlib import Path
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SimpleField,
    SearchableField,
    SearchField,
    VectorSearch,
    VectorSearchProfile,
    HnswAlgorithmConfiguration,
)
from openai import AzureOpenAI
from src.document_generator import DocumentGenerator
from src.embedding import embed_texts

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def normalize_repo_identifier(repo_path: str) -> str:
    """
    ì €ì¥ì†Œ ê²½ë¡œ ë˜ëŠ” URLì„ ì •ê·œí™”ëœ ì‹ë³„ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        repo_path: Git ì €ì¥ì†Œ ê²½ë¡œ ë˜ëŠ” URL

    Returns:
        str: ì •ê·œí™”ëœ ì €ì¥ì†Œ ì‹ë³„ì (í•´ì‹œê°’)
    """
    # URLì¸ ê²½ìš°
    if repo_path.startswith(('http://', 'https://', 'git://', 'ssh://')):
        parsed = urlparse(repo_path)
        # ìŠ¤í‚´ê³¼ netloc, pathë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”
        # .git í™•ì¥ì ì œê±°
        path = parsed.path.rstrip('/').removesuffix('.git')
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}".lower()
    else:
        # ë¡œì»¬ ê²½ë¡œì¸ ê²½ìš°
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™”
        abs_path = Path(repo_path).resolve()
        normalized = str(abs_path).lower()

    # SHA-256 í•´ì‹œë¡œ ë³€í™˜ (ì§§ì€ ë²„ì „ ì‚¬ìš©)
    hash_obj = hashlib.sha256(normalized.encode('utf-8'))
    repo_id = hash_obj.hexdigest()[:16]  # 16ìë¦¬ë¡œ ì¶•ì•½

    logger.debug(f"Normalized '{repo_path}' -> '{normalized}' -> repo_id: {repo_id}")
    return repo_id


class CommitIndexer:
    """Git ì»¤ë°‹ ë°ì´í„°ë¥¼ Azure AI Searchì— ì¸ë±ì‹±í•©ë‹ˆë‹¤."""

    def __init__(
        self,
        search_client: SearchClient,
        index_client: SearchIndexClient,
        openai_client: AzureOpenAI,
        index_name: str
    ):
        """
        Args:
            search_client: Azure AI Search í´ë¼ì´ì–¸íŠ¸
            index_client: Azure AI Search ì¸ë±ìŠ¤ í´ë¼ì´ì–¸íŠ¸
            openai_client: Azure OpenAI í´ë¼ì´ì–¸íŠ¸
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
        """
        self.search_client = search_client
        self.index_client = index_client
        self.openai_client = openai_client
        self.index_name = index_name

    def create_index_if_not_exists(self, vector_dimensions: int = 1536) -> None:
        """
        ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            vector_dimensions: ì„ë² ë”© ë²¡í„° ì°¨ì› (ê¸°ë³¸ê°’: 1536 for text-embedding-3-small)
        """
        try:
            # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            try:
                self.index_client.get_index(self.index_name)
                logger.info(f"Index '{self.index_name}' already exists")
                return
            except:
                pass

            # ì¸ë±ìŠ¤ ìƒì„±
            logger.info(f"Creating index '{self.index_name}'...")

            fields = [
                SimpleField(name="id", type="Edm.String", key=True, filterable=True),
                SimpleField(name="repo_id", type="Edm.String", filterable=True, facetable=True),
                SearchableField(name="repository_path", type="Edm.String", searchable=True),
                SearchableField(name="message", type="Edm.String", searchable=True),
                SimpleField(name="author", type="Edm.String", filterable=True, facetable=True),
                SimpleField(name="date", type="Edm.DateTimeOffset", filterable=True, sortable=True),
                SearchableField(name="files_summary", type="Edm.String", searchable=True),
                SimpleField(name="parent_ids", type="Collection(Edm.String)", filterable=True),
                SimpleField(name="files_changed_count", type="Edm.Int32", filterable=True, sortable=True),
                SimpleField(name="lines_added", type="Edm.Int32", filterable=True, sortable=True),
                SimpleField(name="lines_deleted", type="Edm.Int32", filterable=True, sortable=True),

                # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° í•„ë“œ
                SearchableField(name="change_context_summary", type="Edm.String", searchable=True),
                SearchableField(name="impact_scope", type="Edm.String", searchable=True),
                SearchableField(name="modified_functions", type="Edm.String", searchable=True),
                SearchableField(name="modified_classes", type="Edm.String", searchable=True),
                SimpleField(name="code_complexity", type="Edm.String", filterable=True),
                SimpleField(name="relationship_type", type="Edm.String", filterable=True),
                SimpleField(name="same_author_as_prev", type="Edm.Boolean", filterable=True),

                SearchField(
                    name="content_vector",
                    type="Collection(Edm.Single)",
                    searchable=True,
                    vector_search_dimensions=vector_dimensions,
                    vector_search_profile_name="default-vector-profile"
                ),
            ]

            # Vector search configuration
            vector_search = VectorSearch(
                profiles=[
                    VectorSearchProfile(
                        name="default-vector-profile",
                        algorithm_configuration_name="default-hnsw"
                    )
                ],
                algorithms=[
                    HnswAlgorithmConfiguration(name="default-hnsw")
                ]
            )

            index = SearchIndex(
                name=self.index_name,
                fields=fields,
                vector_search=vector_search
            )

            self.index_client.create_index(index)
            logger.info(f"âœ“ Index '{self.index_name}' created successfully")

        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise

    def _get_existing_ids_for_candidates(self, repo_id: str, candidate_ids: List[str], chunk_size: int = 800) -> Set[str]:
        """
        ì£¼ì–´ì§„ í›„ë³´ ì»¤ë°‹ id ì§‘í•©ì— ëŒ€í•´, ì¸ë±ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” idë§Œ ë°°ì¹˜ë¡œ ì¡°íšŒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        search.in í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ OData í•„í„°ë¡œ í•œ ë²ˆì— ì—¬ëŸ¬ idë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        """
        existing: Set[str] = set()
        if not candidate_ids:
            return existing
        try:
            # ì•ˆì „í•˜ê²Œ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (í•„ë“œ ê¸¸ì´/ì¿¼ë¦¬ ê¸¸ì´ ì œí•œ ëŒ€ë¹„)
            for i in range(0, len(candidate_ids), chunk_size):
                chunk = candidate_ids[i:i + chunk_size]
                ids_joined = ",".join(chunk)
                filter_expr = f"repo_id eq '{repo_id}' and search.in(id, '{ids_joined}', ',')"
                results = self.search_client.search(
                    search_text="*",
                    filter=filter_expr,
                    select=["id"],
                    top=len(chunk)
                )
                for r in results:
                    try:
                        existing.add(r["id"])  # ìµœì†Œ í•„ë“œë§Œ ì ‘ê·¼
                    except Exception:
                        continue
        except Exception as e:
            logger.warning(f"Failed to batch-check existing ids: {e}")
        return existing

    def index_repository(
        self,
        repo_path: str,
        limit: Optional[int] = None,
        since: Optional[str] = None,
        until: Optional[str] = None,
        skip_existing: bool = True,
        skip_offset: int = 0,
        progress_callback: Optional[callable] = None
    ) -> int:
        """
        Git ì €ì¥ì†Œì˜ ì»¤ë°‹ ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤.

        Args:
            repo_path: Git ì €ì¥ì†Œ ê²½ë¡œ ë˜ëŠ” URL
            limit: ì¸ë±ì‹±í•  ìµœëŒ€ ì»¤ë°‹ ìˆ˜ (ê¸°ë³¸ê°’: None = ì „ì²´)
            since: ì‹œì‘ ë‚ ì§œ (ISO 8601 í˜•ì‹, ì˜ˆ: '2024-01-01')
            until: ì¢…ë£Œ ë‚ ì§œ (ISO 8601 í˜•ì‹, ì˜ˆ: '2024-12-31')
            skip_existing: ì´ë¯¸ ì¸ë±ì‹±ëœ ì»¤ë°‹ ê±´ë„ˆë›°ê¸° (ì¦ë¶„ ì¸ë±ì‹±, ê¸°ë³¸ê°’: True)
            skip_offset: HEADë¶€í„° ê±´ë„ˆë›¸ ì»¤ë°‹ ìˆ˜ (ê³¼ê±° ì»¤ë°‹ ì¶”ê°€ ì‹œ ì‚¬ìš©, ê¸°ë³¸ê°’: 0)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (current, total, message)

        Returns:
            int: ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜
        """
        try:
            # ì €ì¥ì†Œ ì‹ë³„ì ìƒì„±
            repo_id = normalize_repo_identifier(repo_path)

            # ğŸ“Š ì¸ë±ì‹± ì¡°ê±´ ìš”ì•½ ë¡œê·¸
            logger.info("=" * 80)
            logger.info("ğŸ“Š INDEXING REQUEST SUMMARY")
            logger.info(f"  ğŸ“ Repository: {repo_path}")
            logger.info(f"  ğŸ”‘ Repo ID: {repo_id}")
            logger.info(f"  ğŸ“ˆ Limit: {limit if limit else 'ALL commits'}")
            logger.info(f"  ğŸ“… Date Range: {since or 'start'} ~ {until or 'end'}")
            logger.info(f"  ğŸ”„ Skip Existing: {skip_existing}")
            logger.info(f"  â­ï¸  Skip Offset: {skip_offset}")
            logger.info("=" * 80)

            # ì´ë¯¸ ì¸ë±ì‹±ëœ ì»¤ë°‹ ID í™•ì¸ ì „ëµ:
            # 1) ì „ì²´ ìŠ¤ìº” ëŒ€ì‹ , í›„ë³´ ì»¤ë°‹ ëª©ë¡ì„ ë¨¼ì € ìˆ˜ì§‘í•œ ë’¤ í•´ë‹¹ idë“¤ë§Œ ë°°ì¹˜ ì¡°íšŒ(search.in)ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            # 2) ê·¸ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ì»¤ë°‹ë§Œ í•„í„°ë§ (ì •í™•/ì €ë¹„ìš©)

            # skip_offsetì´ í¬ë©´ ë¯¸ë¦¬ ì¶©ë¶„í•œ depthë¡œ fetch (ì›ê²© ì €ì¥ì†Œë§Œ)
            if skip_offset > 0 and repo_path.startswith(('http://', 'https://', 'git@', 'ssh://')):
                required_depth = skip_offset + (limit if limit else 100)
                logger.info(f"Skip offset {skip_offset} detected, pre-fetching with depth={required_depth}")

                from src.repo_cache import RepoCloneCache
                cache = RepoCloneCache()
                cache.get_or_clone(repo_path, depth=required_depth)

            # ì»¤ë°‹ ë°ì´í„° ì¶”ì¶œ
            generator = DocumentGenerator(repo_path)
            try:
                commits = generator.get_commits(limit=limit, since=since, until=until, skip=skip_offset)
            finally:
                generator.close()  # íŒŒì¼ í•¸ë“¤ í•´ì œ

            if not commits:
                if skip_offset > 0:
                    logger.warning(f"No commits found with skip_offset={skip_offset}. Shallow clone may not have enough commits.")
                    logger.warning(f"Try with smaller skip_offset or ensure repository is fully fetched.")
                else:
                    logger.warning("No commits found")
                return 0

            # ì¦ë¶„ ìŠ¤í‚µ: í›„ë³´ ì§‘í•©ì— ëŒ€í•´ì„œë§Œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if skip_existing:
                candidate_ids = [c['id'] for c in commits]
                existing_commit_ids = self._get_existing_ids_for_candidates(repo_id, candidate_ids)
                original_count = len(commits)
                commits = [c for c in commits if c['id'] not in existing_commit_ids]
                skipped = original_count - len(commits)
                if skipped > 0:
                    logger.info(f"Skipped {skipped} already indexed commits")

            if not commits:
                logger.info("All commits are already indexed")
                return 0

            logger.info(f"Found {len(commits)} commits to index")

            # ë¬¸ì„œ ì¤€ë¹„
            documents = []
            texts_to_embed = []

            total_commits = len(commits)

            for idx, commit in enumerate(commits):
                # ì§„í–‰ ìƒí™© ì½œë°±
                if progress_callback:
                    try:
                        progress_callback(idx, total_commits, "ë¬¸ì„œ ì¤€ë¹„ ì¤‘")
                    except:
                        pass

                # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ìƒì„± (í–¥ìƒëœ ë¬¸ë§¥ í¬í•¨)
                files_info = [f"{f['file']} ({f['change_type']})" for f in commit['files']]

                # ë³€ê²½ ë¬¸ë§¥ í¬í•¨
                change_context = commit.get('change_context', {})
                function_analysis = commit.get('function_analysis', {})

                # í•¨ìˆ˜ ë³€ê²½ ì •ë³´ ì¶”ê°€
                func_changes = []
                if function_analysis.get('modified_functions'):
                    func_changes.append("Modified: " + ", ".join(
                        [f"{f['name']} in {f['file']}" for f in function_analysis['modified_functions'][:3]]
                    ))
                if function_analysis.get('added_functions'):
                    func_changes.append("Added: " + ", ".join(
                        [f['name'] for f in function_analysis['added_functions'][:3]]
                    ))

                text_content = f"""Commit: {commit['message']}
Author: {commit['author']}
Files: {', '.join(files_info)}
Context: {change_context.get('summary', '')}
Functions: {'; '.join(func_changes) if func_changes else 'No function changes'}"""

                texts_to_embed.append(text_content)

                # í†µê³„ ê³„ì‚°
                lines_added = sum(f.get('lines_added', 0) for f in commit['files'])
                lines_deleted = sum(f.get('lines_deleted', 0) for f in commit['files'])

                # ê¸°ë³¸ ë¬¸ì„œ ë°ì´í„°
                doc = {
                    "id": commit['id'],
                    "repo_id": repo_id,
                    "repository_path": repo_path,
                    "message": commit['message'],
                    "author": commit['author'],
                    "date": commit['date'],
                    "files_summary": ', '.join(files_info),
                    "parent_ids": commit.get('parents', []),
                    "files_changed_count": len(commit['files']),
                    "lines_added": lines_added,
                    "lines_deleted": lines_deleted,
                }

                # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                doc["change_context_summary"] = change_context.get('summary', '')
                doc["impact_scope"] = '; '.join(change_context.get('impact_scope', [])[:5])

                # í•¨ìˆ˜ ë¶„ì„ ë©”íƒ€ë°ì´í„°
                modified_funcs = [f"{f['name']} ({f['file']})"
                                 for f in function_analysis.get('modified_functions', [])[:10]]
                doc["modified_functions"] = ', '.join(modified_funcs) if modified_funcs else ''

                modified_classes = [f"{c['name']} ({c['file']})"
                                   for c in function_analysis.get('modified_classes', [])[:10]]
                doc["modified_classes"] = ', '.join(modified_classes) if modified_classes else ''

                doc["code_complexity"] = function_analysis.get('code_complexity_hint', 'unknown')

                # ì»¤ë°‹ ê´€ê³„ ë©”íƒ€ë°ì´í„°
                relation = commit.get('relation_to_previous')
                if relation:
                    doc["relationship_type"] = relation.get('relationship_type', 'sequential')
                    doc["same_author_as_prev"] = relation.get('same_author', False)
                else:
                    doc["relationship_type"] = 'initial'
                    doc["same_author_as_prev"] = False

                documents.append(doc)

            # ì„ë² ë”© ìƒì„±
            if progress_callback:
                try:
                    progress_callback(total_commits, total_commits, "ì„ë² ë”© ìƒì„± ì¤‘")
                except:
                    pass

            logger.info("Generating embeddings...")
            embeddings = embed_texts(texts_to_embed, self.openai_client)

            # ì„ë² ë”©ì„ ë¬¸ì„œì— ì¶”ê°€
            for doc, embedding in zip(documents, embeddings):
                doc["content_vector"] = embedding

            # ì—…ë¡œë“œ
            if progress_callback:
                try:
                    progress_callback(total_commits, total_commits, "ì—…ë¡œë“œ ì¤‘")
                except:
                    pass

            logger.info(f"Uploading {len(documents)} documents to Azure AI Search...")
            result = self.search_client.upload_documents(documents=documents)

            success_count = sum(1 for r in result if r.succeeded)

            logger.info("=" * 80)
            logger.info(f"âœ… INDEXING COMPLETED")
            logger.info(f"  ğŸ“Š Successfully indexed: {success_count}/{len(documents)} documents")
            logger.info(f"  ğŸ“ Repository: {repo_path}")
            logger.info(f"  ğŸ”‘ Repo ID: {repo_id}")
            logger.info("=" * 80)

            return success_count

        except Exception as e:
            logger.error(f"Failed to index repository: {e}")
            raise

    def delete_index(self) -> None:
        """ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"Deleting index '{self.index_name}'...")
            self.index_client.delete_index(self.index_name)
            logger.info(f"âœ“ Index '{self.index_name}' deleted")
        except Exception as e:
            logger.error(f"Failed to delete index: {e}")
            raise
