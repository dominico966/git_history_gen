"""
Chainlit ê¸°ë°˜ ëŒ€í™”í˜• Git íˆìŠ¤í† ë¦¬ ë¶„ì„ ì±„íŒ… ì•±
í”„ë¡¬í”„íŠ¸ëŠ” êµ¬ì¡°í™”ëœ ê°ì²´ë¡œ ê´€ë¦¬í•˜ë©° ì—¬ëŸ¬ ì¤„ ë¬¸ìì—´(''' or \"\"\") ì‚¬ìš© ê¸ˆì§€
"""

import json
import logging
import os
import sys
from pathlib import Path

import chainlit as cl
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì™€ ì‹¤í–‰ê¸°(ë¶„ë¦¬ëœ ëª¨ë“ˆ)
from src.tool_executor import initialize_clients, execute_tool

from datetime import datetime
TODAY=datetime.today().strftime('%Y-%m-%d')

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì•ˆì „ ì¥ì¹˜: ìµœëŒ€ê°’ ì œí•œ (í† í°/ë¹„ìš© í­íƒ„ ë°©ì§€)
MAX_COMMIT_LIMIT = int(os.getenv("MAX_COMMIT_LIMIT", "1000"))
MAX_SEARCH_TOP = int(os.getenv("MAX_SEARCH_TOP", "50"))
MAX_CONTRIBUTOR_LIMIT = int(os.getenv("MAX_CONTRIBUTOR_LIMIT", "1000"))
DEFAULT_INDEX_LIMIT = int(os.getenv("DEFAULT_INDEX_LIMIT", "500"))

# SocketIO í˜ì´ë¡œë“œ ì œí•œ
MAX_TOOL_RESULT_DISPLAY = 500  # Stepì— í‘œì‹œí•  ìµœëŒ€ ë¬¸ì ìˆ˜
MAX_TOOL_RESULT_TO_LLM = 10000  # LLMì— ì „ë‹¬í•  ìµœëŒ€ ë¬¸ì ìˆ˜
MAX_CONVERSATION_MESSAGES = 20  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ìµœê·¼ Nê°œ ë©”ì‹œì§€

# ----- í”„ë¡¬í”„íŠ¸ ì •ì˜ (ì—¬ëŸ¬ ì¤„ ë¬¸ìì—´ ê¸ˆì§€) -----
SYSTEM_PROMPT_PARTS = [
    # ë‚ ì§œëŠ” ì‹¤í–‰ ì‹œ ì£¼ì…ë¨
    f"Git íˆìŠ¤í† ë¦¬ ë¶„ì„ ì „ë¬¸ê°€. ì˜¤ëŠ˜: {TODAY}",
    "",
    "# í•µì‹¬ ê·œì¹™",
    "- í•œêµ­ì–´ êµ¬ì¡°í™”ëœ ë‹µë³€. í™•ì¸ ì§ˆë¬¸ ê¸ˆì§€",
    "- ë„êµ¬ ë°”ë¡œ ì‹¤í–‰ â†’ ê²°ê³¼ ë¶„ì„ â†’ ëª…í™•í•œ ì„¤ëª…",
    "- ê²€ìƒ‰ì€ ì˜ì–´ë§Œ. ë‹¤ë¥¸ ì–¸ì–´ì‹œ ë²ˆì—­",
    "",
    "# ì¸ë±ì‹± ì „ëµ",
    f"1. ë¶„ì„ ìš”ì²­ì‹œ: list_indexed_repositories â†’ get_commit_count í™•ì¸",
    f"2. ê¸°ë³¸: ìµœê·¼ {DEFAULT_INDEX_LIMIT}ê°œ, HEADë¶€í„° ì‹œì‘, skip_existing=true",
    "3. ì¦ë¶„: skip_offsetìœ¼ë¡œ ê³¼ê±° ì»¤ë°‹ ì¶”ê°€",
    "4. **ì¤‘ìš”**: ì¸ë±ì‹±ìˆ˜ < ì „ì²´ìˆ˜ â†’ ì¶”ê°€ í•„ìš”. 'ì „ë¶€' ìš”ì²­ì‹œ 100% ì™„ë£Œê¹Œì§€",
    "5. ê·œëª¨ë³„: ~500(ê¸°ë³¸), 500~1000(skip_offset), 1000+(ë‚ ì§œë²”ìœ„)",
    "",
    "# ì¦ë¶„ ì¸ë±ì‹± ê²°ê³¼ í•´ì„",
    "- ë¡œê·¸ì— 'Skipped N already indexed commits' í‘œì‹œ â†’ **ì •ìƒ ë™ì‘**",
    "- ì´ë¯¸ ì¸ë±ì‹±ëœ ì»¤ë°‹ì€ ìë™ìœ¼ë¡œ ê±´ë„ˆë›°ê³ , ìƒˆë¡œìš´ ì»¤ë°‹ë§Œ ì¸ë±ì‹±í•¨",
    "- ì˜ˆ: 436ê°œ ì¶”ì¶œ â†’ 100ê°œ ê±´ë„ˆëœ€ â†’ 336ê°œ ì¸ë±ì‹± = **336ê°œ ì¶”ê°€ë¨**",
    "- ì‚¬ìš©ìì—ê²ŒëŠ” **ìƒˆë¡œ ì¶”ê°€ëœ ê°œìˆ˜**ì™€ **ì „ì²´ ì¸ë±ì‹± í˜„í™©**ì„ í•¨ê»˜ ì„¤ëª…",
    "",
    "# 'ì „ì²´' ì¸ë±ì‹± ìš”ì²­ ì²˜ë¦¬",
    "- **'ì „ë¶€', 'ë‹¤ í•´', 'ì „ì²´', 'ëª¨ë“ ' ë“±ì˜ ìš”ì²­ ì‹œ**:",
    "  1. get_commit_countë¡œ ì „ì²´ ì»¤ë°‹ ê°œìˆ˜(N) í™•ì¸",
    "  2. list_indexed_repositoriesë¡œ ì´ë¯¸ ì¸ë±ì‹±ëœ ê°œìˆ˜(M) í™•ì¸",
    "  3. index_repository í˜¸ì¶œ ì‹œ limit=N ì„¤ì • (ì¦ë¶„ ì¸ë±ì‹± ìë™ ì ìš©ë¨)",
    "  4. ì™„ë£Œ í›„ ì‹¤ì œ ì¸ë±ì‹±ëœ ê°œìˆ˜ í™•ì¸ ë° ê²€ì¦",
    "- **ì ˆëŒ€ limit ì—†ì´ í˜¸ì¶œí•˜ì§€ ë§ ê²ƒ** (ê¸°ë³¸ê°’ 500ê°œë§Œ ì²˜ë¦¬ë¨)",
    "",
    "# ìì—°ì–´ ì¸ë±ì‹± ìš”ì²­",
    "- **'ì˜¬í•´', '2025ë…„'**: since=2025-01-01, until=2025-12-31 ìë™ ì„¤ì •",
    "- **'ìµœê·¼'**: limit=500 ê¶Œì¥",
    "- **ë‚ ì§œ ì§€ì •**: since/until íŒŒë¼ë¯¸í„° ì‚¬ìš© (YYYY-MM-DD)",
    "- UIì—ì„œë„ í‚¤ì›Œë“œ ì…ë ¥ ê°€ëŠ¥: 'ì˜¬í•´', 'ì „ì²´', 'ìµœê·¼', ë‚ ì§œ",
    "",
    "# í•„ìˆ˜ íŒë‹¨ ì›ì¹™",
    "- **ì¶”ì¸¡ ê¸ˆì§€**: get_commit_count â†” list_indexed_repositories ë¹„êµ í•„ìˆ˜",
    "- ë¶€ë¶„ ì¸ë±ì‹± â†’ ì¶”ê°€ ì‘ì—…, ì™„ì „ ì¸ë±ì‹± â†’ \"ì´ë¯¸ ìˆìŒ\" ê°€ëŠ¥",
    "- ë‚ ì§œë²”ìœ„ í›„ ì‹¤ì œ ê²°ê³¼ ê²€ì¦",
    "",
    "# âš ï¸ commit_sha ì‚¬ìš© ê·œì¹™",
    "- **search_commits ê²°ê³¼ì˜ commit_idëŠ” ì‹¤ì œ ì»¤ë°‹ SHA í•´ì‹œì…ë‹ˆë‹¤**",
    "- get_commit_diff, read_file_from_commit ë“±ì„ í˜¸ì¶œí•  ë•Œ:",
    "  âœ… DO: commit_id ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: 'a1b2c3d4e5f6')",
    "  âŒ DON'T: ì»¤ë°‹ ë©”ì‹œì§€ë¥¼ ì‚¬ìš© (ì˜ˆ: 'feat: ìƒˆ ê¸°ëŠ¥')",
    "- ì˜ˆì‹œ:",
    "  search_commits â†’ commit_id: 'abc123def456'",
    "  get_commit_diff(commit_sha='abc123def456') âœ…",
    "  get_commit_diff(commit_sha='feat: ìƒˆ ê¸°ëŠ¥') âŒ",
    "",
    "# ë„êµ¬",
    "- search_commits: ìë™ UI í™•ì¸",
    "- index_repository: ëŒ€ìš©ëŸ‰ì‹œ ìë™ UI í™•ì¸",
    "- ë‚ ì§œ: YYYY-MM-DD í˜•ì‹",
]


def _build_system_prompt(today: str) -> str:
    """SYSTEM_PROMPT_PARTSë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì£¼ì…í•´ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"""
    # ì—¬ëŸ¬ ì¤„ ë¬¸ìì—´ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë¼ì¸ ë‹¨ìœ„ë¡œ ê²°í•©
    lines = []
    for part in SYSTEM_PROMPT_PARTS:
        lines.append(part.replace("{TODAY}", today))
    return "\n".join(lines)


# ëª¨ë“ˆ import ì‹œì ì˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸(í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©). ë™ì  ë‚ ì§œëŠ” get_system_prompt()ë¥¼ ê¶Œì¥
try:
    from datetime import datetime
    SYSTEM_PROMPT = _build_system_prompt(datetime.now().strftime("%Y-%m-%d"))
except Exception:
    # ë‚ ì§œ ìƒì„± ì‹¤íŒ¨ ì‹œì—ë„ ìƒìˆ˜ê°€ ì¡´ì¬í•˜ë„ë¡ fallback
    SYSTEM_PROMPT = _build_system_prompt("0000-00-00")


def get_system_prompt() -> str:
    """ì••ì¶•ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í•µì‹¬ ê·œì¹™ë§Œ í¬í•¨ (ë™ì  ë‚ ì§œ ì£¼ì…)"""
    from datetime import datetime
    today = datetime.now().strftime("%Y-%m-%d")
    return _build_system_prompt(today)

# ë„êµ¬ ë“±ë¡ì€ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤: `src.tool_registry` (ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë° AVAILABLE_TOOLS)
from src.tool_registry import AVAILABLE_TOOLS


# Chainlit í˜ì´ì§€ ë©”íƒ€ë°ì´í„° is intentionally not set at import time to avoid
# accessing Chainlit registry during plain imports. It can be configured in
# runtime hooks if desired.



# ìŠ¤íƒ€í„° ì œì•ˆ (ì•± ì‹œì‘ ì‹œ í‘œì‹œ)
@cl.set_starters
async def set_starters():
    """ì´ˆê¸° ì‹œì‘ í™”ë©´ì— í‘œì‹œí•  ìŠ¤íƒ€í„° ì œì•ˆ - Chainlit starters ê¸°ëŠ¥

    Icons8 MCPë¥¼ í†µí•´ ì „ë¬¸ì ì¸ ì•„ì´ì½˜ ì ìš©:
    - Database (1476): ì €ì¥ì†Œ ì¸ë±ì‹±
    - Commit Git (33279): ì»¤ë°‹ ìš”ì•½
    - Users/Group (102261): ê¸°ì—¬ì ë¶„ì„
    - Bug (417): ë²„ê·¸ ìˆ˜ì • ì»¤ë°‹
    """
    return [
        cl.Starter(
            label="ì €ì¥ì†Œ ì¸ë±ì‹± ì‹œì‘",
            message="í˜„ì¬ ì €ì¥ì†Œì˜ ì»¤ë°‹ íˆìŠ¤í† ë¦¬ë¥¼ ì¸ë±ì‹±í•´ì£¼ì„¸ìš”. ì €ì¥ì†Œ ê·œëª¨ë¥¼ ë¨¼ì € í™•ì¸í•˜ê³  ì ì ˆí•œ ê°œìˆ˜ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.",
            icon="/public/icons/icon_db_1476_64.png",
        ),
        cl.Starter(
            label="ìµœê·¼ ì»¤ë°‹ ìš”ì•½",
            message="ìµœê·¼ 10ê°œì˜ ì»¤ë°‹ì„ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë³€ê²½ì‚¬í•­ê³¼ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            icon="/public/icons/icon_commit_33279_64.png",
        ),
        cl.Starter(
            label="ê¸°ì—¬ì í™œë™ ë¶„ì„",
            message="ì €ì¥ì†Œì˜ ê¸°ì—¬ìë³„ í™œë™ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ëˆ„ê°€ ê°€ì¥ ë§ì´ ê¸°ì—¬í–ˆëŠ”ì§€, ì£¼ìš” ë‹´ë‹¹ ì˜ì—­ì€ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.",
            icon="/public/icons/icon_users_102261_64.png",
        ),
        cl.Starter(
            label="ë²„ê·¸ ìˆ˜ì • ì»¤ë°‹ ì°¾ê¸°",
            message="ë²„ê·¸ ìˆ˜ì •ê³¼ ê´€ë ¨ëœ ì»¤ë°‹ë“¤ì„ ì°¾ì•„ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”. ì–´ë–¤ ë²„ê·¸ë“¤ì´ ì£¼ë¡œ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            icon="/public/icons/icon_bug_417_64.png",
        ),
    ]


@cl.on_chat_start
async def on_chat_start():
    """ì±„íŒ… ì‹œì‘ ì‹œ ì´ˆê¸°í™” - Chainlit chat life cycleì˜ on_chat_start í›…

    Note: í™˜ì˜ ë©”ì‹œì§€ëŠ” chainlit.mdì—ì„œ ì²˜ë¦¬ë¨
    ì´ í•¨ìˆ˜ëŠ” ì„¸ì…˜ ì´ˆê¸°í™”ë§Œ ìˆ˜í–‰í•˜ì—¬ startersê°€ ë¨¼ì € ë³´ì´ë„ë¡ í•¨
    """
    try:
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        openai_client, search_client, index_client = initialize_clients()

        # ì„¸ì…˜ ë³€ìˆ˜ ì„¤ì •
        cl.user_session.set("openai_client", openai_client)
        cl.user_session.set("search_client", search_client)
        cl.user_session.set("index_client", index_client)
        cl.user_session.set("conversation_history", [
            {"role": "system", "content": get_system_prompt()}
        ])
        cl.user_session.set("is_processing", False)

        logger.info("Chat session started - clients initialized")

    except Exception as e:
        logger.error(f"Failed to start chat: {e}")
        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë§Œ ë©”ì‹œì§€ í‘œì‹œ
        await cl.Message(content=f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}\n\ní˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.").send()


@cl.on_message
async def on_message(message: cl.Message):
    """ë©”ì‹œì§€ ì²˜ë¦¬ - Chainlit chat life cycleì˜ on_message í›…"""
    try:
        # ì²˜ë¦¬ ì¤‘ì¸ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
        is_processing = cl.user_session.get("is_processing")
        if is_processing:
            await cl.Message(content="ì´ì „ ìš”ì²­ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.").send()
            return

        # ì²˜ë¦¬ ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •
        cl.user_session.set("is_processing", True)

        openai_client = cl.user_session.get("openai_client")
        search_client = cl.user_session.get("search_client")
        index_client = cl.user_session.get("index_client")
        conversation_history = cl.user_session.get("conversation_history")

        if not openai_client or not search_client or not index_client:
            cl.user_session.set("is_processing", False)
            await cl.Message(content="ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.").send()
            return

        user_message = message.content
        conversation_history.append({"role": "user", "content": user_message})

        # ëŒ€í™” ê¸°ë¡ ê¸¸ì´ ì œí•œ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ìµœê·¼ Nê°œ ë©”ì‹œì§€)
        max_history_length = MAX_CONVERSATION_MESSAGES + 1  # +1 for system prompt
        if len(conversation_history) > max_history_length:
            system_msg = conversation_history[0]
            recent_messages = conversation_history[-(MAX_CONVERSATION_MESSAGES):]
            conversation_history = [system_msg] + recent_messages
            logger.info(f"Conversation history trimmed to {len(conversation_history)} messages")

        # ë¶„ì„ ì¤‘ ë©”ì‹œì§€ë¥¼ ë¨¼ì € í‘œì‹œ (Stepë“¤ì´ ì´ ì•„ë˜ì—ì„œ ì‹¤í–‰ë¨)
        msg = cl.Message(content="â³ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        await msg.send()

        max_iterations = 10
        iteration = 0
        has_tool_result = False  # ë„êµ¬ ì‹¤í–‰ í›„ ìµœì¢… ì‘ë‹µ ë³´ì¥ìš© í”Œë˜ê·¸

        while iteration < max_iterations:
            iteration += 1

            # ğŸ”§ ì „ì²´ ë‹¨ê³„ë¥¼ ê°ì‹¸ëŠ” ë¶€ëª¨ Step
            async with cl.Step(name=f"ğŸ”§ ì‘ì—… ìˆ˜í–‰ (ë‹¨ê³„ {iteration})", type="run", show_input=False) as parent_step:
                try:
                    # ğŸ’­ ë¶„ì„ ë‹¨ê³„ (ìì‹ Step)
                    async with cl.Step(name="ğŸ’­ ë¶„ì„ ì¤‘...", parent_id=parent_step.id, type="llm", show_input=False) as analysis_step:
                        response = openai_client.chat.completions.create(
                            model=os.getenv("AZURE_OPENAI_MODEL", "gpt-4o-mini"),
                            messages=conversation_history,
                            tools=AVAILABLE_TOOLS,
                            tool_choice="auto",
                            temperature=0.7,
                            max_tokens=1000,
                            stream=False  # ë„êµ¬ ì„ íƒì„ ìœ„í•´ non-streaming
                        )

                        assistant_message = response.choices[0].message

                        # LLMì˜ ìƒê° í‘œì‹œ (ê°„ê²°í•˜ê²Œ)
                        if assistant_message.tool_calls:
                            tool_names = [tc.function.name for tc in assistant_message.tool_calls]
                            analysis_step.output = f"ğŸ”§ ë„êµ¬ ì„ íƒ: {', '.join(tool_names)}"
                        elif assistant_message.content:
                            analysis_step.output = "âœ… ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ"

                    # ë„êµ¬ í˜¸ì¶œ ì—†ì´ ìµœì¢… ì‘ë‹µë§Œ ìˆëŠ” ê²½ìš° - ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±
                    if not assistant_message.tool_calls:
                        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
                        msg.content = ""

                        streaming_response = openai_client.chat.completions.create(
                            model=os.getenv("AZURE_OPENAI_MODEL", "gpt-4o-mini"),
                            messages=conversation_history,
                            temperature=0.7,
                            max_tokens=1000,
                            stream=True
                        )

                        final_content = ""
                        for chunk in streaming_response:
                            # ì•ˆì „í•˜ê²Œ choicesì™€ delta ì²´í¬
                            if chunk.choices and len(chunk.choices) > 0:
                                delta = chunk.choices[0].delta
                                if delta and hasattr(delta, 'content') and delta.content:
                                    token = delta.content
                                    final_content += token
                                    await msg.stream_token(token)

                        await msg.update()

                        if final_content:
                            conversation_history.append({
                                "role": "assistant",
                                "content": final_content
                            })
                        parent_step.output = "âœ… ì‘ë‹µ ì™„ë£Œ"
                        break

                    conversation_history.append({
                        "role": "assistant",
                        "content": assistant_message.content or "",
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "type": "function",
                                "function": {
                                    "name": tc.function.name,
                                    "arguments": tc.function.arguments
                                }
                            }
                            for tc in assistant_message.tool_calls
                        ]
                    })

                    for tool_call in assistant_message.tool_calls:
                        tool_name = tool_call.function.name
                        tool_args = json.loads(tool_call.function.arguments)

                        # ğŸ› ï¸ ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„ (ìì‹ Step)
                        async with cl.Step(name=f"ğŸ› ï¸ {tool_name}", parent_id=parent_step.id, type="tool", show_input=False) as tool_step:
                            tool_result = await execute_tool(
                                tool_name=tool_name,
                                arguments=tool_args,
                                openai_client=openai_client,
                                search_client=search_client,
                                index_client=index_client
                            )

                            # ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
                            has_tool_result = True

                            # Step ì¶œë ¥ì€ ê°„ê²°í•˜ê²Œ
                            if len(tool_result) > MAX_TOOL_RESULT_DISPLAY:
                                tool_step.output = f"âœ… ì™„ë£Œ (ê²°ê³¼ {len(tool_result):,}ì)"
                            else:
                                tool_step.output = f"âœ… ì™„ë£Œ"

                            # LLMì—ê²Œ ì „ë‹¬í•  ê²°ê³¼ëŠ” ì œí•œ
                            truncated_result = tool_result[:MAX_TOOL_RESULT_TO_LLM]
                            if len(tool_result) > MAX_TOOL_RESULT_TO_LLM:
                                truncated_result += f"\n\n...(ì´ {len(tool_result)}ì ì¤‘ {MAX_TOOL_RESULT_TO_LLM}ì í‘œì‹œ)"
                                logger.warning(f"Tool result truncated: {len(tool_result)} -> {MAX_TOOL_RESULT_TO_LLM}")

                            conversation_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": truncated_result
                            })

                    parent_step.output = "âœ… ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ"

                    # ğŸ”„ ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ìŒ í–‰ë™ ê²°ì • (tool_calls í™•ì¸ì„ ìœ„í•´ non-streaming)
                    logger.info("Checking next action after tool execution...")
                    next_response = openai_client.chat.completions.create(
                        model=os.getenv("AZURE_OPENAI_MODEL", "gpt-4o-mini"),
                        messages=conversation_history,
                        tools=AVAILABLE_TOOLS,
                        tool_choice="auto",
                        temperature=0.7,
                        max_tokens=1000,
                        stream=False
                    )

                    next_message = next_response.choices[0].message

                    # ì¶”ê°€ ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ í˜„ì¬ iterationì—ì„œ ê³„ì† ì‹¤í–‰
                    if next_message.tool_calls:
                        logger.info(f"More tool calls needed: {[tc.function.name for tc in next_message.tool_calls]}")

                        # assistant ë©”ì‹œì§€ ì¶”ê°€
                        conversation_history.append({
                            "role": "assistant",
                            "content": next_message.content or "",
                            "tool_calls": [
                                {
                                    "id": tc.id,
                                    "type": "function",
                                    "function": {
                                        "name": tc.function.name,
                                        "arguments": tc.function.arguments
                                    }
                                }
                                for tc in next_message.tool_calls
                            ]
                        })

                        # ì¶”ê°€ ë„êµ¬ë“¤ì„ í˜„ì¬ iterationì—ì„œ ì‹¤í–‰
                        for tool_call in next_message.tool_calls:
                            tool_name = tool_call.function.name
                            tool_args = json.loads(tool_call.function.arguments)

                            async with cl.Step(name=f"ğŸ› ï¸ {tool_name} (ì¶”ê°€)", parent_id=parent_step.id, type="tool", show_input=False) as extra_tool_step:
                                extra_tool_result = await execute_tool(
                                    tool_name=tool_name,
                                    arguments=tool_args,
                                    openai_client=openai_client,
                                    search_client=search_client,
                                    index_client=index_client
                                )

                                truncated_extra_result = extra_tool_result[:MAX_TOOL_RESULT_TO_LLM]
                                if len(extra_tool_result) > MAX_TOOL_RESULT_TO_LLM:
                                    truncated_extra_result += f"\n\n...(ì´ {len(extra_tool_result)}ì ì¤‘ {MAX_TOOL_RESULT_TO_LLM}ì í‘œì‹œ)"

                                conversation_history.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "content": truncated_extra_result
                                })

                                extra_tool_step.output = "âœ… ì™„ë£Œ"

                        parent_step.output = "ğŸ”„ ì¶”ê°€ ë„êµ¬ ì‹¤í–‰ í•„ìš”"
                        # ë‹¤ìŒ iterationìœ¼ë¡œ ê³„ì†
                        continue

                    # í…ìŠ¤íŠ¸ ì‘ë‹µì´ ìˆìœ¼ë©´ conversation_historyì— ì¶”ê°€í•˜ê³  ë£¨í”„ ì¢…ë£Œ
                    if next_message.content:
                        logger.info("Final response ready, adding to history")
                        conversation_history.append({
                            "role": "assistant",
                            "content": next_message.content
                        })
                        parent_step.output = "âœ… ì™„ë£Œ"
                        has_tool_result = False  # ì´ë¯¸ ì‘ë‹µì´ ìˆìœ¼ë¯€ë¡œ ë°–ì—ì„œ ìƒì„±í•˜ì§€ ì•ŠìŒ

                        # msgì— ë‚´ìš© í‘œì‹œ
                        msg.content = next_message.content
                        await msg.update()
                        break  # ë£¨í”„ ì¢…ë£Œ

                except Exception as e:
                    logger.error(f"Error in iteration {iteration}: {e}")
                    parent_step.output = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    break

        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ì‹œì—ë§Œ ê²½ê³  ë©”ì‹œì§€
        if iteration >= max_iterations and not msg.content:
            msg.content = "âš ï¸ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìš”ì²­ì„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            await msg.update()

        cl.user_session.set("conversation_history", conversation_history)

    except Exception as e:
        logger.error(f"Message handling error: {e}")
        await cl.Message(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}").send()
    finally:
        # ì²˜ë¦¬ ì™„ë£Œ í”Œë˜ê·¸ í•´ì œ
        cl.user_session.set("is_processing", False)


@cl.on_stop
async def on_stop():
    """ì‚¬ìš©ìê°€ ì¤‘ì§€ ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œ - Chainlit chat life cycleì˜ on_stop í›…"""
    logger.info("User requested to stop the task")
    cl.user_session.set("is_processing", False)
    await cl.Message(content="â¸ï¸ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.").send()


@cl.on_chat_end
async def on_chat_end():
    """ì±„íŒ… ì„¸ì…˜ ì¢…ë£Œ ì‹œ - Chainlit chat life cycleì˜ on_chat_end í›…"""
    logger.info("Chat session ended")
    # ì„¸ì…˜ ì •ë¦¬ (í•„ìš”ì‹œ)
    cl.user_session.set("is_processing", False)
    cl.user_session.set("conversation_history", None)
    cl.user_session.set("openai_client", None)
    cl.user_session.set("search_client", None)
    cl.user_session.set("index_client", None)


# í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„ ìœ„í•œ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ì‹¬ë³¼ ì œê³µ

def start() -> None:
    """Chainlit ì•± ì‹œì‘ìš© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸(í…ŒìŠ¤íŠ¸ì—ì„œ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸)."""
    logger.info("start() called - Chainlit entry placeholder")


def main() -> None:
    """ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸(í…ŒìŠ¤íŠ¸ì—ì„œ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸)."""
    logger.info("main() called - entry placeholder")


if __name__ == "__main__":
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))




